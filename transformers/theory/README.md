# Transformers â€” Theory

This section is for **core concepts and mental models** behind transformer architectures.  
These notes aim for clarity, intuition, and structural understanding â€” before touching code.
å›¾çµè¯´ï¼šä¸€ä¸ªæƒ³æ³•æ³¨å…¥åï¼Œå¯èƒ½åƒç´å¼¦ä¸€æ ·è¡°å‡ï¼Œä¹Ÿå¯èƒ½åƒæ¥è¿‘ä¸´ç•Œçš„æ ¸å †ä¸€æ ·è‡ªæ¿€å¢é•¿ã€‚
Transformer çš„æ®‹å·®æµï¼ˆresidual streamï¼‰+ å¤šå±‚éçº¿æ€§ï¼šè¾“å…¥ token çš„ä¿¡æ¯ä¼šåœ¨å±‚ä¸å±‚ä¹‹é—´è¢«åå¤å˜æ¢ã€ç»„åˆã€æ”¾å¤§ï¼Œå½¢æˆâ€œçº§è”â€çš„å†…éƒ¨è¡¨å¾ã€‚
è‡ªå›å½’ç”Ÿæˆï¼šæ¨¡å‹è‡ªå·±ç”Ÿæˆçš„ token åˆå›åˆ°è¾“å…¥é‡Œï¼ˆcontextï¼‰ï¼Œä¼šå½¢æˆå¤–éƒ¨å¯è§çš„â€œæ€è·¯å»¶å±•â€ã€‚

## ğŸ“˜ Planned Topics

### 1. Tokenization
- Why models operate on tokens, not characters or words
- Byte-Pair Encoding (BPE) and other tokenization methods  
- How text becomes a sequence of integers  

### 2. Embeddings
#### **WTE â€” Word Token Embedding**
- Mapping token IDs â†’ dense vectors  
- Why embedding space encodes semantic similarity

#### **WPE â€” Word Position Embedding**
- How transformers learn positional information  
- Absolute vs relative position embeddings  
- Why attention is â€œorder-agnosticâ€ without them  

### 3. Self-Attention (Q, K, V)
- What problem attention solves  
- Intuition for Query / Key / Value  
- Why attention is â€œsoft lookupâ€  
- Dot-product attention: QKáµ€ â†’ softmax â†’ V  
- Masked attention for autoregressive models  

### 4. Multi-Head Attention & Residual Connections
- Why multiple heads capture different patterns  
- How heads are projected and concatenated  
- Why residual/skip connections stabilize training  
- Add â†’ Norm sequence and its importance  

### 5. Transformer Block Structure
- LayerNorm placement (â€œpre-normâ€ vs â€œpost-normâ€)  
- MLP feed-forward network (two linear layers + nonlinearity)  
- Quick overview of common activation functions (GELU, ReLU)  
- Full block = Attention â†’ MLP â†’ Skip connections â†’ LayerNorm  

### 6. Logits
- What logits actually represent  
- Why they are unnormalized scores  
- Relationship between logits and probability distributions  

### 7. Softmax
- Converting logits into probabilities  
- Temperature scaling  
- Why softmax is used for attention & decoding  
- Numerical stability tricks (subtract max, etc.)  

---

## ğŸ¯ Purpose of This Section
To build a **clean, complete transformer intuition** that makes it easier to:

- understand what models *actually compute*  
- debug / reason about model behavior  
- implement simplified transformer modules from scratch  
- meaningfully use transformers in real tasks (see Practice)  
- build your own tiny attention / tiny block (see Implementation)

This is the *mental map* before touching code.
